import streamlit as st
import pandas as pd
import os
import logging
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import tempfile

# Ğ†Ğ¼Ğ¿Ğ¾Ñ€Ñ‚ÑƒÑ”Ğ¼Ğ¾ Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ— Ğ· Ğ¼Ğ¾Ğ´ÑƒĞ»Ñ–Ğ²
from src.ai_analyzer import train_category_model, predict_category, train_anomaly_model, predict_anomaly, save_model, load_model
try:
    from src.ai_analyzer import add_anomaly_features
except ImportError:
    add_anomaly_features = None
from src.utils import load_and_preprocess_data, preprocess_transactions, clean_text

# ĞĞ°Ğ»Ğ°ÑˆÑ‚ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ»Ğ¾Ğ³ÑƒĞ²Ğ°Ğ½Ğ½Ñ
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Ğ”Ğ¾Ğ¿Ğ¾Ğ¼Ñ–Ğ¶Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾Ğ³Ğ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ


def adapt_uploaded_file(uploaded_file):
    """
    ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ÑƒÑ” Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ñƒ Ñ‚Ğ¸Ğ¼Ñ‡Ğ°ÑĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸.
    """
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_path = tmp_file.name
        logger.info(
            f"Ğ¡Ñ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ¾ Ñ‚Ğ¸Ğ¼Ñ‡Ğ°ÑĞ¾Ğ²Ğ¸Ğ¹ Ñ„Ğ°Ğ¹Ğ»: {tmp_path}")
        return tmp_path
    except Exception as e:
        logger.error(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ñƒ: {e}")
        st.error(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
        return None

# Ğ¤ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ»Ñ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ°Ğ±Ğ¾ Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹


@st.cache_resource
def load_or_train_models():
    """
    Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” Ğ°Ğ±Ğ¾ Ñ‚Ñ€ĞµĞ½ÑƒÑ” Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ— Ñ‚Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹.
    """
    model_cat_path = 'models/category_model.pkl'
    model_anomaly_path = 'models/anomaly_model.pkl'

    os.makedirs('models', exist_ok=True)
    os.makedirs('data', exist_ok=True)

    category_model, vectorizer, label_encoder = None, None, None
    anomaly_model, anomaly_scaler = None, None

    if not (os.path.exists(model_cat_path) and os.path.exists(model_anomaly_path)):
        st.info(
            "ĞœĞ¾Ğ´ĞµĞ»Ñ– Ğ²Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ–. Ğ Ğ¾Ğ·Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°Ñ”Ğ¼Ğ¾ Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ...")
        data_path = os.path.join('data', 'transactions.csv')
        if not os.path.exists(data_path):
            st.error(
                "Ğ¤Ğ°Ğ¹Ğ» data/transactions.csv Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾. Ğ¡Ñ‚Ğ²Ğ¾Ñ€Ñ–Ñ‚ÑŒ Ğ¹Ğ¾Ğ³Ğ¾.")
            logger.error("Ğ¤Ğ°Ğ¹Ğ» Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ğ¹ Ğ²Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ–Ğ¹.")
            return None, None, None, None, None

        df_raw = load_and_preprocess_data(data_path)
        if df_raw is None:
            st.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ´Ğ°Ğ½Ğ¸Ñ….")
            logger.error(
                "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ğ´Ğ°Ğ½Ñ–.")
            return None, None, None, None, None

        keywords = ['Ğ°Ñ‚Ğ±', 'comfy', 'mono', 'ÑÑƒĞ¿ĞµÑ€Ğ¼Ğ°Ñ€ĞºĞµÑ‚',
                    'ĞºĞ°Ğ²Ğ°', 'Ğ°Ğ¿Ñ‚ĞµĞºĞ°', 'Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ°']
        df_processed = preprocess_transactions(
            df_raw.copy(), language='ukrainian', keep_keywords=keywords)
        if df_processed is None:
            st.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….")
            logger.error("ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ¸Ñ‚Ğ¸ Ğ´Ğ°Ğ½Ñ–.")
            return None, None, None, None, None

        required_cols = ['cleaned_description', 'category', 'amount',
                         'hour_of_day', 'day_of_week', 'month', 'is_weekend']
        if not all(col in df_processed.columns for col in required_cols):
            st.error(
                "Ğ”Ğ°Ğ½Ñ– Ğ½Ğµ Ğ¼Ñ–ÑÑ‚ÑÑ‚ÑŒ Ğ½ĞµĞ¾Ğ±Ñ…Ñ–Ğ´Ğ½Ğ¸Ñ… ÑÑ‚Ğ¾Ğ²Ğ¿Ñ†Ñ–Ğ².")
            logger.error("Ğ’Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ– Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ñ– ÑÑ‚Ğ¾Ğ²Ğ¿Ñ†Ñ–.")
            return None, None, None, None, None

        category_model, vectorizer, _, label_encoder = train_category_model(
            df_processed, model_type='xgboost', use_transformers=False)
        if category_model:
            save_model(category_model, model_cat_path, {
                       'vectorizer': vectorizer, 'label_encoder': label_encoder, 'use_transformers': False})
            st.success(
                "ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ— Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ°.")
            logger.info(
                "ĞœĞ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ— Ğ½Ğ°Ğ²Ñ‡ĞµĞ½Ğ°.")
        else:
            st.error(
                "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ—.")
            logger.error(
                "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ½Ğ°Ğ²Ñ‡Ğ¸Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ—.")
            return None, None, None, None, None

        anomaly_features = ['amount', 'hour_of_day',
                            'day_of_week', 'month', 'is_weekend']
        anomaly_model, anomaly_scaler = train_anomaly_model(
            df_processed, anomaly_features, contamination=None)
        if anomaly_model:
            save_model(anomaly_model, model_anomaly_path,
                       {'scaler': anomaly_scaler})
            st.success("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹ Ğ·Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ°.")
            logger.info("ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹ Ğ½Ğ°Ğ²Ñ‡ĞµĞ½Ğ°.")
        else:
            st.error(
                "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ‚Ñ€ĞµĞ½ÑƒĞ²Ğ°Ğ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ– Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹.")
            logger.error(
                "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ½Ğ°Ğ²Ñ‡Ğ¸Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹.")
            return None, None, None, None, None

    else:
        st.info("Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾ Ñ–ÑĞ½ÑƒÑÑ‡Ñ– Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–...")
        category_data = load_model(model_cat_path)
        anomaly_data = load_model(model_anomaly_path)
        if category_data and anomaly_data:
            category_model, meta_cat = category_data
            anomaly_model, meta_anomaly = anomaly_data
            vectorizer = meta_cat.get('vectorizer')
            label_encoder = meta_cat.get('label_encoder')
            anomaly_scaler = meta_anomaly.get('scaler')
            logger.info("ĞœĞ¾Ğ´ĞµĞ»Ñ– ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾.")
        else:
            st.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.")
            logger.error(
                "ĞĞµ Ğ²Ğ´Ğ°Ğ»Ğ¾ÑÑ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»Ñ–.")
            return None, None, None, None, None

    return category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler

# Ğ“Ğ¾Ğ»Ğ¾Ğ²Ğ½Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ Ğ´Ğ¾Ğ´Ğ°Ñ‚ĞºÑƒ


def main_dashboard():
    st.set_page_config(layout="wide", page_title="Smart Transaction Analyzer")
    st.title("ğŸ’° ĞĞ½Ğ°Ğ»Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ¢Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ğ¹ Monobank")
    st.markdown("Ğ”Ğ¾Ğ´Ğ°Ñ‚Ğ¾Ğº Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ğ¹, ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ— Ñ‚Ğ° Ğ²Ğ¸ÑĞ²Ğ»ĞµĞ½Ğ½Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹.")

    # Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ Ñ„Ğ°Ğ¹Ğ»Ñƒ
    st.header("Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ñ‚Ğµ Ñ„Ğ°Ğ¹Ğ» Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ğ¹")
    uploaded_file = st.file_uploader(
        "Ğ’Ğ¸Ğ±ĞµÑ€Ñ–Ñ‚ÑŒ CSV-Ñ„Ğ°Ğ¹Ğ»", type="csv")

    df_transactions = None
    if uploaded_file:
        with st.spinner("ĞĞ±Ñ€Ğ¾Ğ±ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ñƒ..."):
            tmp_path = adapt_uploaded_file(uploaded_file)
            if tmp_path:
                df_processed = load_and_preprocess_data(tmp_path)
                os.unlink(tmp_path)
                if df_processed is not None:
                    keywords = ['Ğ°Ñ‚Ğ±', 'comfy', 'mono', 'ÑÑƒĞ¿ĞµÑ€Ğ¼Ğ°Ñ€ĞºĞµÑ‚',
                                'ĞºĞ°Ğ²Ğ°', 'Ğ°Ğ¿Ñ‚ĞµĞºĞ°', 'Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ°']
                    df_transactions = preprocess_transactions(
                        df_processed.copy(), language='ukrainian', keep_keywords=keywords)
                    st.success(
                        "Ğ”Ğ°Ğ½Ñ– ÑƒÑĞ¿Ñ–ÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ¾!")
                    st.write("ĞŸĞµÑ€ÑˆÑ– 5 Ñ€ÑĞ´ĞºÑ–Ğ²:",
                             df_transactions.head())
                else:
                    st.error("ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¸ Ğ´Ğ°Ğ½Ğ¸Ñ….")
            else:
                st.error(
                    "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ĞµĞ½Ğ½Ñ– Ñ„Ğ°Ğ¹Ğ»Ñƒ.")
    else:
        st.info(
            "Ğ‘ÑƒĞ´ÑŒ Ğ»Ğ°ÑĞºĞ°, Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ñ‚Ğµ CSV-Ñ„Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ.")

    if df_transactions is not None:
        # Ğ†Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
        with st.spinner("ĞŸÑ–Ğ´Ğ³Ğ¾Ñ‚Ğ¾Ğ²ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹..."):
            category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler = load_or_train_models()
            if not all([category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler]):
                st.error(
                    "ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ñ–Ğ½Ñ–Ñ†Ñ–Ğ°Ğ»Ñ–Ğ·Ğ°Ñ†Ñ–Ñ— Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹.")
                return
            st.success("ĞœĞ¾Ğ´ĞµĞ»Ñ– Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ–!")

        # ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ´Ğ°Ğ½Ğ¸Ñ…
        with st.spinner("ĞĞ½Ğ°Ğ»Ñ–Ğ· Ñ‚Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ğ¹..."):
            df_transactions['predicted_category'] = df_transactions['cleaned_description'].apply(
                lambda x: predict_category(x, category_model, vectorizer, label_encoder, use_transformers=False))

            if add_anomaly_features is not None:
                df_transactions = add_anomaly_features(df_transactions.copy())
            anomaly_features = ['amount', 'hour_of_day',
                                'day_of_week', 'month', 'is_weekend']
            if any(col not in df_transactions.columns for col in anomaly_features):
                st.error(
                    "Ğ’Ñ–Ğ´ÑÑƒÑ‚Ğ½Ñ– Ğ½ĞµĞ¾Ğ±Ñ…Ñ–Ğ´Ğ½Ñ– Ğ¾Ğ·Ğ½Ğ°ĞºĞ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ğ¹.")
                return

            if df_transactions[anomaly_features].isnull().any().any():
                df_transactions[anomaly_features] = df_transactions[anomaly_features].fillna(
                    df_transactions[anomaly_features].mean())

            try:
                scaled_data = anomaly_scaler.transform(
                    df_transactions[anomaly_features])
                df_transactions['is_anomaly_predicted'] = anomaly_model.predict(
                    scaled_data) == -1
                st.success("ĞĞ½Ğ°Ğ»Ñ–Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
            except Exception as e:
                st.error(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ° Ğ°Ğ½Ğ°Ğ»Ñ–Ğ·Ñƒ: {e}")
                logger.error(f"ĞŸĞ¾Ğ¼Ğ¸Ğ»ĞºĞ°: {e}")
                return

        # Ğ¤Ñ–Ğ»ÑŒÑ‚Ñ€Ğ¸
        st.header("Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸ ĞĞ½Ğ°Ğ»Ñ–Ğ·Ñƒ")
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("ĞŸĞ¾Ñ‡Ğ°Ñ‚ĞºĞ¾Ğ²Ğ° Ğ´Ğ°Ñ‚Ğ°", df_transactions['date'].min(
            ).date() if not df_transactions['date'].empty else datetime.now().date())
        with col2:
            end_date = st.date_input("ĞšÑ–Ğ½Ñ†ĞµĞ²Ğ° Ğ´Ğ°Ñ‚Ğ°", df_transactions['date'].max(
            ).date() if not df_transactions['date'].empty else datetime.now().date())

        filtered_df = df_transactions[(df_transactions['date'] >= pd.Timestamp(start_date)) &
                                      (df_transactions['date'] <= pd.Timestamp(end_date).replace(hour=23, minute=59, second=59))]

        # Ğ’Ğ¸Ğ²ĞµĞ´ĞµĞ½Ğ½Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ–
        st.subheader("Ğ¢Ñ€Ğ°Ğ½Ğ·Ğ°ĞºÑ†Ñ–Ñ— Ğ· Ğ¿Ñ€Ğ¾Ğ³Ğ½Ğ¾Ğ·Ğ°Ğ¼Ğ¸")
        st.dataframe(filtered_df[['date', 'amount', 'description',
                     'category', 'predicted_category', 'is_anomaly_predicted']])

        # Ğ“Ñ€Ğ°Ñ„Ñ–ĞºĞ¸ Ğ· Ğ¼ĞµĞ½ÑˆĞ¸Ğ¼Ğ¸ Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€Ğ°Ğ¼Ğ¸
        if not filtered_df.empty:
            st.subheader("Ğ Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ» ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ğ¹")
            # Ğ—Ğ¼ĞµĞ½ÑˆĞµĞ½Ğ¾ Ğ· (10, 6)
            fig_cat, ax_cat = plt.subplots(figsize=(5, 3))
            sns.barplot(x=filtered_df['predicted_category'].value_counts().index,
                        y=filtered_df['predicted_category'].value_counts().values, ax=ax_cat, palette='viridis')
            ax_cat.set_title("Ğ Ğ¾Ğ·Ğ¿Ğ¾Ğ´Ñ–Ğ» ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ğ¹")
            ax_cat.set_xlabel("ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ñ–Ñ")
            ax_cat.set_ylabel("ĞšÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_cat)

            st.subheader("ĞĞ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ñ—")
            fig_anom, ax_anom = plt.subplots(
                figsize=(6, 3))  # Ğ—Ğ¼ĞµĞ½ÑˆĞµĞ½Ğ¾ Ğ· (12, 6)
            sns.scatterplot(data=filtered_df, x='date', y='amount', hue='is_anomaly_predicted',
                            palette={True: 'red', False: 'blue'}, alpha=0.7, ax=ax_anom)
            ax_anom.set_title("ĞĞ½Ğ¾Ğ¼Ğ°Ğ»Ñ–Ñ— Ğ·Ğ° ÑÑƒĞ¼Ğ¾Ñ")
            ax_anom.set_xlabel("Ğ”Ğ°Ñ‚Ğ°")
            ax_anom.set_ylabel("Ğ¡ÑƒĞ¼Ğ°")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_anom)

            st.subheader("Ğ”Ğ¸Ğ½Ğ°Ğ¼Ñ–ĞºĞ° Ğ²Ğ¸Ñ‚Ñ€Ğ°Ñ‚")
            # Ğ—Ğ¼ĞµĞ½ÑˆĞµĞ½Ğ¾ Ğ· (12, 6)
            fig_dyn, ax_dyn = plt.subplots(figsize=(6, 3))
            filtered_df['date_month'] = filtered_df['date'].dt.to_period('M')
            monthly_data = filtered_df.groupby(
                'date_month')['amount'].sum().reset_index()
            monthly_data['date_month'] = monthly_data['date_month'].astype(str)
            sns.lineplot(data=monthly_data, x='date_month',
                         y='amount', marker='o', ax=ax_dyn)
            ax_dyn.set_title("ĞœÑ–ÑÑÑ‡Ğ½Ğ° Ğ´Ğ¸Ğ½Ğ°Ğ¼Ñ–ĞºĞ°")
            ax_dyn.set_xlabel("ĞœÑ–ÑÑÑ†ÑŒ")
            ax_dyn.set_ylabel("Ğ¡ÑƒĞ¼Ğ°")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_dyn)

        # Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚
        if st.button("Ğ•ĞºÑĞ¿Ğ¾Ñ€Ñ‚ÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¸"):
            csv_data = filtered_df[['date', 'amount', 'description', 'category',
                                    'predicted_category', 'is_anomaly_predicted']].to_csv(index=False, encoding='utf-8-sig')
            st.download_button(label="Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ CSV", data=csv_data,
                               file_name="analyzed_transactions.csv", mime="text/csv")


if __name__ == "__main__":
    main_dashboard()
