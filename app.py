import streamlit as st
import pandas as pd
import os
import logging
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import tempfile

# –Ü–º–ø–æ—Ä—Ç—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—ó –∑ –º–æ–¥—É–ª—ñ–≤
from src.ai_analyzer import train_category_model, predict_category, train_anomaly_model, predict_anomaly, save_model, load_model
try:
    from src.ai_analyzer import add_anomaly_features
except ImportError:
    add_anomaly_features = None
from src.utils import load_and_preprocess_data, preprocess_transactions, clean_text

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# –î–æ–ø–æ–º—ñ–∂–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ–≥–æ —Ñ–∞–π–ª—É


def adapt_uploaded_file(uploaded_file):
    """
    –ö–æ–Ω–≤–µ—Ä—Ç—É—î –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∏–π —Ñ–∞–π–ª —É —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª –¥–ª—è –æ–±—Ä–æ–±–∫–∏.
    """
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
            tmp_file.write(uploaded_file.read())
            tmp_path = tmp_file.name
        logger.info(
            f"–°—Ç–≤–æ—Ä–µ–Ω–æ —Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ñ–∞–π–ª: {tmp_path}")
        return tmp_path
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ —Ñ–∞–π–ª—É: {e}")
        st.error(f"–ü–æ–º–∏–ª–∫–∞: {e}")
        return None

# –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∞–±–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª–µ–π


@st.cache_resource
def load_or_train_models():
    """
    –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î –∞–±–æ —Ç—Ä–µ–Ω—É—î –º–æ–¥–µ–ª—ñ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ –∞–Ω–æ–º–∞–ª—ñ–π.
    """
    model_cat_path = 'models/category_model.pkl'
    model_anomaly_path = 'models/anomaly_model.pkl'

    os.makedirs('models', exist_ok=True)
    os.makedirs('data', exist_ok=True)

    category_model, vectorizer, label_encoder = None, None, None
    anomaly_model, anomaly_scaler = None, None

    if not (os.path.exists(model_cat_path) and os.path.exists(model_anomaly_path)):
        st.info(
            "–ú–æ–¥–µ–ª—ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ. –†–æ–∑–ø–æ—á–∏–Ω–∞—î–º–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è...")
        data_path = os.path.join('data', 'transactions.csv')
        if not os.path.exists(data_path):
            st.error(
                "–§–∞–π–ª data/transactions.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ. –°—Ç–≤–æ—Ä—ñ—Ç—å –π–æ–≥–æ.")
            logger.error("–§–∞–π–ª —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π –≤—ñ–¥—Å—É—Ç–Ω—ñ–π.")
            return None, None, None, None, None

        df_raw = load_and_preprocess_data(data_path)
        if df_raw is None:
            st.error("–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö.")
            logger.error(
                "–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –¥–∞–Ω—ñ.")
            return None, None, None, None, None

        keywords = ['–∞—Ç–±', 'comfy', 'mono', '—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç',
                    '–∫–∞–≤–∞', '–∞–ø—Ç–µ–∫–∞', '–∑–∞—Ä–ø–ª–∞—Ç–∞']
        df_processed = preprocess_transactions(
            df_raw.copy(), language='ukrainian', keep_keywords=keywords)
        if df_processed is None:
            st.error("–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö.")
            logger.error("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±—Ä–æ–±–∏—Ç–∏ –¥–∞–Ω—ñ.")
            return None, None, None, None, None

        required_cols = ['cleaned_description', 'category', 'amount',
                         'hour_of_day', 'day_of_week', 'month', 'is_weekend']
        if not all(col in df_processed.columns for col in required_cols):
            st.error(
                "–î–∞–Ω—ñ –Ω–µ –º—ñ—Å—Ç—è—Ç—å –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö —Å—Ç–æ–≤–ø—Ü—ñ–≤.")
            logger.error("–í—ñ–¥—Å—É—Ç–Ω—ñ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Å—Ç–æ–≤–ø—Ü—ñ.")
            return None, None, None, None, None

        category_model, vectorizer, _, label_encoder = train_category_model(
            df_processed, model_type='xgboost', use_transformers=False)
        if category_model:
            save_model(category_model, model_cat_path, {
                       'vectorizer': vectorizer, 'label_encoder': label_encoder, 'use_transformers': False})
            st.success(
                "–ú–æ–¥–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–∞.")
            logger.info(
                "–ú–æ–¥–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó –Ω–∞–≤—á–µ–Ω–∞.")
        else:
            st.error(
                "–ü–æ–º–∏–ª–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó.")
            logger.error(
                "–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó.")
            return None, None, None, None, None

        anomaly_features = ['amount', 'hour_of_day',
                            'day_of_week', 'month', 'is_weekend']
        anomaly_model, anomaly_scaler = train_anomaly_model(
            df_processed, anomaly_features, contamination=None)
        if anomaly_model:
            save_model(anomaly_model, model_anomaly_path,
                       {'scaler': anomaly_scaler})
            st.success("–ú–æ–¥–µ–ª—å –∞–Ω–æ–º–∞–ª—ñ–π –∑–±–µ—Ä–µ–∂–µ–Ω–∞.")
            logger.info("–ú–æ–¥–µ–ª—å –∞–Ω–æ–º–∞–ª—ñ–π –Ω–∞–≤—á–µ–Ω–∞.")
        else:
            st.error(
                "–ü–æ–º–∏–ª–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –∞–Ω–æ–º–∞–ª—ñ–π.")
            logger.error(
                "–ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–≤—á–∏—Ç–∏ –º–æ–¥–µ–ª—å –∞–Ω–æ–º–∞–ª—ñ–π.")
            return None, None, None, None, None

    else:
        st.info("–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —ñ—Å–Ω—É—é—á—ñ –º–æ–¥–µ–ª—ñ...")
        category_data = load_model(model_cat_path)
        anomaly_data = load_model(model_anomaly_path)
        if category_data and anomaly_data:
            category_model, meta_cat = category_data
            anomaly_model, meta_anomaly = anomaly_data
            vectorizer = meta_cat.get('vectorizer')
            label_encoder = meta_cat.get('label_encoder')
            anomaly_scaler = meta_anomaly.get('scaler')
            logger.info("–ú–æ–¥–µ–ª—ñ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")
        else:
            st.error("–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª–µ–π.")
            logger.error(
                "–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ –º–æ–¥–µ–ª—ñ.")
            return None, None, None, None, None

    return category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler

# –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–æ–¥–∞—Ç–∫—É


def main_dashboard():
    st.set_page_config(layout="wide", page_title="Smart Transaction Analyzer")
    st.title("üí∞ –ê–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä –¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π Monobank")
    st.markdown("–î–æ–¥–∞—Ç–æ–∫ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π, –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—ó —Ç–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ–π.")

    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ñ–∞–π–ª—É
    st.header("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–∞–π–ª —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π")
    uploaded_file = st.file_uploader(
        "–í–∏–±–µ—Ä—ñ—Ç—å CSV-—Ñ–∞–π–ª", type="csv")

    df_transactions = None
    if uploaded_file:
        with st.spinner("–û–±—Ä–æ–±–∫–∞ —Ñ–∞–π–ª—É..."):
            tmp_path = adapt_uploaded_file(uploaded_file)
            if tmp_path:
                df_processed = load_and_preprocess_data(tmp_path)
                os.unlink(tmp_path)
                if df_processed is not None:
                    keywords = ['–∞—Ç–±', 'comfy', 'mono', '—Å—É–ø–µ—Ä–º–∞—Ä–∫–µ—Ç',
                                '–∫–∞–≤–∞', '–∞–ø—Ç–µ–∫–∞', '–∑–∞—Ä–ø–ª–∞—Ç–∞']
                    df_transactions = preprocess_transactions(
                        df_processed.copy(), language='ukrainian', keep_keywords=keywords)
                    st.success(
                        "–î–∞–Ω—ñ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")
                    st.write("–ü–µ—Ä—à—ñ 5 —Ä—è–¥–∫—ñ–≤:",
                             df_transactions.head())
                else:
                    st.error("–ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏ –¥–∞–Ω–∏—Ö.")
            else:
                st.error(
                    "–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ —Ñ–∞–π–ª—É.")
    else:
        st.info(
            "–ë—É–¥—å –ª–∞—Å–∫–∞, –∑–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ CSV-—Ñ–∞–π–ª –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É.")

    if df_transactions is not None:
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª–µ–π
        with st.spinner("–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –º–æ–¥–µ–ª–µ–π..."):
            category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler = load_or_train_models()
            if not all([category_model, vectorizer, label_encoder, anomaly_model, anomaly_scaler]):
                st.error(
                    "–ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –º–æ–¥–µ–ª–µ–π.")
                return
            st.success("–ú–æ–¥–µ–ª—ñ –≥–æ—Ç–æ–≤—ñ!")

        # –ê–Ω–∞–ª—ñ–∑ –¥–∞–Ω–∏—Ö
        with st.spinner("–ê–Ω–∞–ª—ñ–∑ —Ç—Ä–∞–Ω–∑–∞–∫—Ü—ñ–π..."):
            df_transactions['predicted_category'] = df_transactions['cleaned_description'].apply(
                lambda x: predict_category(x, category_model, vectorizer, label_encoder, use_transformers=False))

            if add_anomaly_features is not None:
                df_transactions = add_anomaly_features(df_transactions.copy())
            anomaly_features = ['amount', 'hour_of_day',
                                'day_of_week', 'month', 'is_weekend']
            if any(col not in df_transactions.columns for col in anomaly_features):
                st.error(
                    "–í—ñ–¥—Å—É—Ç–Ω—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω—ñ –æ–∑–Ω–∞–∫–∏ –¥–ª—è –∞–Ω–æ–º–∞–ª—ñ–π.")
                return

            if df_transactions[anomaly_features].isnull().any().any():
                df_transactions[anomaly_features] = df_transactions[anomaly_features].fillna(
                    df_transactions[anomaly_features].mean())

            try:
                scaled_data = anomaly_scaler.transform(
                    df_transactions[anomaly_features])
                df_transactions['is_anomaly_predicted'] = anomaly_model.predict(
                    scaled_data) == -1
                st.success("–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            except Exception as e:
                st.error(f"–ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}")
                logger.error(f"–ü–æ–º–∏–ª–∫–∞: {e}")
                return

        # –§—ñ–ª—å—Ç—Ä–∏
        st.header("–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –ê–Ω–∞–ª—ñ–∑—É")
        col1, col2 = st.columns(2)
        with col1:
            start_date = st.date_input("–ü–æ—á–∞—Ç–∫–æ–≤–∞ –¥–∞—Ç–∞", df_transactions['date'].min(
            ).date() if not df_transactions['date'].empty else datetime.now().date())
        with col2:
            end_date = st.date_input("–ö—ñ–Ω—Ü–µ–≤–∞ –¥–∞—Ç–∞", df_transactions['date'].max(
            ).date() if not df_transactions['date'].empty else datetime.now().date())

        filtered_df = df_transactions[(df_transactions['date'] >= pd.Timestamp(start_date)) &
                                      (df_transactions['date'] <= pd.Timestamp(end_date).replace(hour=23, minute=59, second=59))]

        # –í–∏–≤–µ–¥–µ–Ω–Ω—è —Ç–∞–±–ª–∏—Ü—ñ
        st.subheader("–¢—Ä–∞–Ω–∑–∞–∫—Ü—ñ—ó –∑ –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏")
        st.dataframe(filtered_df[['date', 'amount', 'description',
                     'category', 'predicted_category', 'is_anomaly_predicted']])

        # –ì—Ä–∞—Ñ—ñ–∫–∏ –∑ –º–µ–Ω—à–∏–º–∏ —Ä–æ–∑–º—ñ—Ä–∞–º–∏
        if not filtered_df.empty:
            st.subheader("–†–æ–∑–ø–æ–¥—ñ–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π")
            # –ó–º–µ–Ω—à–µ–Ω–æ –∑ (10, 6)
            fig_cat, ax_cat = plt.subplots(figsize=(5, 3))
            sns.barplot(x=filtered_df['predicted_category'].value_counts().index,
                        y=filtered_df['predicted_category'].value_counts().values, ax=ax_cat, palette='viridis')
            ax_cat.set_title("–†–æ–∑–ø–æ–¥—ñ–ª –∫–∞—Ç–µ–≥–æ—Ä—ñ–π")
            ax_cat.set_xlabel("–ö–∞—Ç–µ–≥–æ—Ä—ñ—è")
            ax_cat.set_ylabel("–ö—ñ–ª—å–∫—ñ—Å—Ç—å")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_cat)

            st.subheader("–ê–Ω–æ–º–∞–ª—ñ—ó")
            fig_anom, ax_anom = plt.subplots(
                figsize=(6, 3))  # –ó–º–µ–Ω—à–µ–Ω–æ –∑ (12, 6)
            sns.scatterplot(data=filtered_df, x='date', y='amount', hue='is_anomaly_predicted',
                            palette={True: 'red', False: 'blue'}, alpha=0.7, ax=ax_anom)
            ax_anom.set_title("–ê–Ω–æ–º–∞–ª—ñ—ó –∑–∞ —Å—É–º–æ—é")
            ax_anom.set_xlabel("–î–∞—Ç–∞")
            ax_anom.set_ylabel("–°—É–º–∞")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_anom)

            st.subheader("–î–∏–Ω–∞–º—ñ–∫–∞ –≤–∏—Ç—Ä–∞—Ç")
            # –ó–º–µ–Ω—à–µ–Ω–æ –∑ (12, 6)
            fig_dyn, ax_dyn = plt.subplots(figsize=(6, 3))
            filtered_df['date_month'] = filtered_df['date'].dt.to_period('M')
            monthly_data = filtered_df.groupby(
                'date_month')['amount'].sum().reset_index()
            monthly_data['date_month'] = monthly_data['date_month'].astype(str)
            sns.lineplot(data=monthly_data, x='date_month',
                         y='amount', marker='o', ax=ax_dyn)
            ax_dyn.set_title("–ú—ñ—Å—è—á–Ω–∞ –¥–∏–Ω–∞–º—ñ–∫–∞")
            ax_dyn.set_xlabel("–ú—ñ—Å—è—Ü—å")
            ax_dyn.set_ylabel("–°—É–º–∞")
            plt.xticks(rotation=45, ha='right')
            st.pyplot(fig_dyn)

        # –ï–∫—Å–ø–æ—Ä—Ç
        if st.button("–ï–∫—Å–ø–æ—Ä—Ç—É–≤–∞—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏"):
            csv_data = filtered_df[['date', 'amount', 'description', 'category',
                                    'predicted_category', 'is_anomaly_predicted']].to_csv(index=False, encoding='utf-8-sig')
            st.download_button(label="–ó–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ CSV", data=csv_data,
                               file_name="analyzed_transactions.csv", mime="text/csv")


if __name__ == "__main__":
    main_dashboard()
